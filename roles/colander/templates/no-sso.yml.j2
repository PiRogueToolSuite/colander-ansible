# {{ ansible_managed }}

volumes:
  production_traefik: 
    driver: local
    driver_opts:
      type: none
      device: {{ stack.services.traefik.volumes.config }}
      o: bind 
  production_traefik_logs: 
    driver: local
    driver_opts:
      type: none
      device: {{ stack.services.traefik.volumes.logs }}
      o: bind 
  production_colander_postgres_data: 
    driver: local
    driver_opts:
      type: none
      device: {{ stack.services.colander_postgres.volumes.data }}
      o: bind 
  production_colander_postgres_backups: 
    driver: local
    driver_opts:
      type: none
      device: {{ stack.services.colander_postgres.volumes.backups }}
      o: bind 
{% if flavor.use_threatr %}
  production_threatr_postgres_data: 
    driver: local
    driver_opts:
      type: none
      device: {{ stack.services.threatr_postgres.volumes.data }}
      o: bind 
  production_threatr_postgres_backups: 
    driver: local
    driver_opts:
      type: none
      device: {{ stack.services.threatr_postgres.volumes.backups }}
      o: bind 
{% endif %}
  production_minio_data: 
    driver: local
    driver_opts:
      type: none
      device: {{ stack.services.minio.volumes.data }}
      o: bind 
  production_es_data: 
    driver: local
    driver_opts:
      type: none
      device: {{ stack.services.elasticsearch.volumes.data }}
      o: bind 

networks:
  frontend:
  backend:
    driver: bridge

services:
  # ----------------------------
  # Colander
  # ----------------------------
  # Web front 
  colander-front:
    image: ghcr.io/piroguetoolsuite/colander:{{stack.services.colander.version}}
    restart: unless-stopped
    logging:
      driver: journald
      options:
        labels: colander.service.name,colander.service.type
    healthcheck:
      test: [ "CMD", "grep", "-q", ":1388", "/proc/net/tcp" ]
      interval: 30s
      timeout: 2s
      retries: 5
    networks:
      - backend
    depends_on:
      colander-postgres:
        condition: service_healthy
      colander-worker:
        condition: service_started
      elasticsearch:
        condition: service_healthy
      minio:
        condition: service_healthy
      redis:
        condition: service_healthy
{% if flavor.use_threatr %}
      threatr-front:
        condition: service_started
{% endif %}
{% if flavor.use_mandolin %}
      mandolin:
        condition: service_healthy
{% endif %}
{% if flavor.use_playwright %}
      playwright:
        condition: service_started
{% endif %}
{% if flavor.use_cyberchef %}
      cyberchef:
        condition: service_started
{% endif %}
    command: /start
    env_file:
      - {{ dot_env_files_dir }}/{{ stack.base.dot_env_file}}
      - {{ dot_env_files_dir }}/{{ stack.services.minio.dot_env_file}}
      - {{ dot_env_files_dir }}/{{ stack.services.colander.dot_env_file}}
      - {{ dot_env_files_dir }}/{{ stack.services.colander_postgres.dot_env_file}}
    labels:
      com.centurylinklabs.watchtower.enable: true
      colander.service.name: colander-front
      colander.service.type: web

  # Database
  colander-postgres:
    build:
      context: .
      dockerfile: ./compose/postgres/Dockerfile
    restart: unless-stopped
    logging:
      driver: journald
      options:
        labels: colander.service.name,colander.service.type
    healthcheck:
      test: [ "CMD-SHELL", "pg_isready" ]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - backend
    volumes:
      - production_colander_postgres_data:/var/lib/postgresql/data
      - production_colander_postgres_backups:/backups
    env_file:
      - {{ dot_env_files_dir }}/{{ stack.services.colander_postgres.dot_env_file}}
    labels:
      colander.service.name: colander-postgres
      colander.service.type: database

  # Workers
  colander-worker:
    image: ghcr.io/piroguetoolsuite/colander:{{stack.services.colander.version}}
    restart: unless-stopped
    logging:
      driver: journald
      options:
        labels: colander.service.name,colander.service.type
    networks:
      - backend
    depends_on:
      colander-postgres:
        condition: service_healthy
      elasticsearch:
        condition: service_healthy
      minio:
        condition: service_healthy
      redis:
        condition: service_healthy
{% if flavor.use_mandolin %}
      mandolin:
        condition: service_healthy
{% endif %}
{% if flavor.use_playwright %}
      playwright:
        condition: service_started
{% endif %}
    command: /start-worker
    env_file:
      - {{ dot_env_files_dir }}/{{ stack.base.dot_env_file}}
      - {{ dot_env_files_dir }}/{{ stack.services.minio.dot_env_file}}
      - {{ dot_env_files_dir }}/{{ stack.services.colander.dot_env_file}}
      - {{ dot_env_files_dir }}/{{ stack.services.colander_postgres.dot_env_file}}
    labels:
      com.centurylinklabs.watchtower.enable: true
      colander.service.name: colander-worker
      colander.service.type: worker

  # ----------------------------
  # Threatr: {{ flavor.use_threatr }}
  # ----------------------------
{% if flavor.use_threatr %}
  # Web front 
  threatr-front:
    image: ghcr.io/piroguetoolsuite/threatr:{{stack.services.threatr.version}}
    restart: unless-stopped
    logging:
      driver: journald
      options:
        labels: colander.service.name,colander.service.type
    healthcheck:
      test: [ "CMD", "grep", "-q", ":1388", "/proc/net/tcp" ]
      interval: 30s
      timeout: 2s
      retries: 5
    networks:
      - backend
    depends_on:
      threatr-postgres:
        condition: service_healthy
      threatr-worker:
        condition: service_started
      elasticsearch:
        condition: service_healthy
      redis:
        condition: service_healthy
    command: /start
    env_file:
      - {{ dot_env_files_dir }}/{{ stack.base.dot_env_file}}
      - {{ dot_env_files_dir }}/{{ stack.services.threatr.dot_env_file}}
      - {{ dot_env_files_dir }}/{{ stack.services.threatr_postgres.dot_env_file}}
    labels:
      com.centurylinklabs.watchtower.enable: true
      colander.service.name: threatr-front
      colander.service.type: web

  # Workers
  threatr-worker:
    image: ghcr.io/piroguetoolsuite/threatr:{{stack.services.threatr.version}}
    restart: unless-stopped
    logging:
      driver: journald
      options:
        labels: colander.service.name,colander.service.type
    networks:
      - backend
    depends_on:
      threatr-postgres:
        condition: service_healthy
      elasticsearch:
        condition: service_healthy
      redis:
        condition: service_healthy
    command: /start-worker
    env_file:
      - {{ dot_env_files_dir }}/{{ stack.base.dot_env_file}}
      - {{ dot_env_files_dir }}/{{ stack.services.threatr.dot_env_file}}
      - {{ dot_env_files_dir }}/{{ stack.services.threatr_postgres.dot_env_file}}
    labels:
      com.centurylinklabs.watchtower.enable: true
      colander.service.name: threatr-worker
      colander.service.type: worker

  # Database
  threatr-postgres:
    build:
      context: .
      dockerfile: ./compose/postgres/Dockerfile
    restart: unless-stopped
    logging:
      driver: journald
      options:
        labels: colander.service.name,colander.service.type
    healthcheck:
      test: [ "CMD-SHELL", "pg_isready" ]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - backend
    volumes:
      - production_threatr_postgres_data:/var/lib/postgresql/data
      - production_threatr_postgres_backups:/backups
    env_file:
      - {{ dot_env_files_dir }}/{{ stack.services.threatr_postgres.dot_env_file}}
    labels:
      colander.service.name: threatr-postgres
      colander.service.type: database
{% endif %}

  # ----------------------------
  # Load balancers - Reverse proxy
  # ----------------------------
  traefik:
    build:
      context: .
      dockerfile: ./compose/traefik/Dockerfile
    restart: unless-stopped
    logging:
      driver: journald
      options:
        labels: colander.service.name,colander.service.type
    networks:
      - frontend
      - backend
    depends_on:
      colander-front:
        condition: service_healthy
{% if flavor.use_threatr %}
      threatr-front:
        condition: service_healthy
{% endif %}
{% if flavor.use_cyberchef %}
      cyberchef:
        condition: service_started
{% endif %}
    volumes:
      - production_traefik:/etc/traefik/acme
      - production_traefik_logs:/logs/
    ports:
      - "0.0.0.0:80:80"
      - "0.0.0.0:443:443"
    labels:
      colander.service.name: traefik
      colander.service.type: proxy

  # ----------------------------
  # Mandolin: {{ flavor.use_mandolin }}
  # ----------------------------
{% if flavor.use_mandolin %}
  tika:
    image: apache/tika:{{stack.services.tika.version}}
    restart: unless-stopped
    logging:
      driver: journald
      options:
        labels: colander.service.name,colander.service.type
    networks:
      - backend
    healthcheck:
      test: [ "CMD", "wget", "-O", "/dev/null", "http://localhost:9998/tika" ]
      interval: 30s
      timeout: 20s
      retries: 3
    labels:
      com.centurylinklabs.watchtower.enable: true
      colander.service.name: tika
      colander.service.type: backend

  mandolin:
    image: ghcr.io/piroguetoolsuite/mandolin:{{stack.services.mandolin.version}}
    restart: unless-stopped
    logging:
      driver: journald
      options:
        labels: colander.service.name,colander.service.type
    networks:
      - backend
    depends_on:
      tika:
        condition: service_healthy
    healthcheck:
      test: [ "CMD", "wget", "-O", "/dev/null", "http://localhost:8000/" ]
      interval: 30s
      timeout: 20s
      retries: 3
    environment:
      - TIKA_URL=http://tika:9998/
      - MAX_FILE_SIZE=250000000  # 250MB
    labels:
      com.centurylinklabs.watchtower.enable: true
      colander.service.name: mandolin
      colander.service.type: backend
{% endif %}

  # ----------------------------
  # Cyberchef: {{ flavor.use_cyberchef }}
  # ----------------------------
{% if flavor.use_cyberchef %}
  cyberchef:
    image: mpepping/cyberchef:{{stack.services.cyberchef.version}}
    restart: unless-stopped
    logging:
      driver: journald
      options:
        labels: colander.service.name,colander.service.type
    networks:
      - backend
    labels:
      com.centurylinklabs.watchtower.enable: false
      colander.service.name: cyberchef
      colander.service.type: web
{% endif %}

  # ----------------------------
  # Playwright: {{ flavor.use_playwright }}
  # ----------------------------
{% if flavor.use_playwright %}
  playwright:
    image: ghcr.io/piroguetoolsuite/playwright-rest-api:{{stack.services.playwright.version}}
    restart: unless-stopped
    logging:
      driver: journald
      options:
        labels: colander.service.name,colander.service.type
    networks:
      - backend
    labels:
      com.centurylinklabs.watchtower.enable: true
      colander.service.name: playwright
      colander.service.type: backend
{% endif %}

  # ----------------------------
  # Elasticsearch
  # ----------------------------
  elasticsearch:
    image: elasticsearch:{{stack.services.elasticsearch.version}}
    restart: unless-stopped
    logging:
      driver: journald
      options:
        labels: colander.service.name,colander.service.type
    networks:
      - backend
    volumes:
      - production_es_data:/usr/share/elasticsearch/data
    healthcheck:
      test: [ "CMD-SHELL", "curl -s -X GET http://localhost:9200/_cluster/health?pretty | grep status | grep -q '\\(green\\|yellow\\)'" ]
      interval: 10s
      timeout: 10s
      retries: 24
    environment:
      - logger.level=WARN
      - discovery.type=single-node
      - xpack.security.enabled=false
      - path.repo=/usr/share/elasticsearch/data/snapshots
      - ES_JAVA_OPTS=-Xms{{stack.services.elasticsearch.xms}} -Xmx{{stack.services.elasticsearch.xmx}}
    labels:
      colander.service.name: elasticsearch
      colander.service.type: database

  # ----------------------------
  # Minio
  # ----------------------------
  minio:
    image: quay.io/minio/minio:{{stack.services.minio.version}}
    volumes:
      - production_minio_data:/data
    command: server /data
    restart: unless-stopped
    logging:
      driver: journald
      options:
        labels: colander.service.name,colander.service.type
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:9000/minio/health/live" ]
      interval: 30s
      timeout: 20s
      retries: 3
    networks:
      - backend
    env_file:
      - {{ dot_env_files_dir }}/{{ stack.services.minio.dot_env_file}}
    labels:
      colander.service.name: minio
      colander.service.type: storage

  # ----------------------------
  # Watchtower
  # ----------------------------
  watchtower:
    image: containrrr/watchtower:{{stack.services.watchtower.version}}
    restart: unless-stopped
    logging:
      driver: journald
      options:
        labels: colander.service.name,colander.service.type
    networks:
      - backend
    volumes:
      - /etc/timezone:/etc/timezone:ro
      - {{docker_socket_path}}:/var/run/docker.sock:ro
    environment:
      - WATCHTOWER_CLEANUP=true
      - WATCHTOWER_LABEL_ENABLE=true
      - WATCHTOWER_INCLUDE_RESTARTING=true
      {{ stack.services.watchtower.environment | to_nice_yaml | indent(6) }}
    labels:
      com.centurylinklabs.watchtower.enable: true
      colander.service.name: watchtower
      colander.service.type: docker

  # ----------------------------
  # Redis
  # ----------------------------
  redis:
    image: redis:6
    restart: unless-stopped
    logging:
      driver: journald
      options:
        labels: colander.service.name,colander.service.type
    healthcheck:
      test: ["CMD-SHELL", "redis-cli ping | grep PONG"]
      interval: 2s
      timeout: 3s
      retries: 5
    networks:
      - backend
    labels:
      colander.service.name: redis
      colander.service.type: database
